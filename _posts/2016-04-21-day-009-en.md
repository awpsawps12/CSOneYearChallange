---
layout: post
title:  Day 009
ref: day-009
lang: en
date:   2016-04-21 00:00:00 +0800
course_id: 1
---

![](/images/Day009-en.png)

The designing programs goals are,

1. Return the correct answer on all legal inputs (Most important)
2. Performs the computation efficiency (Sometime critical, always valuable to understand and optimize)

Computational complexity related to two things (1) how much time will it take a program to run and (2) how much memory will it need to run. It is also important to balance to computational complexity and conceptual complexity.

**Computational Complexity**

*Measure complexity* using time is inaccurate (How long will program take to run?) because the answer depends on,

- Machine speed (Measuring time in term of number of basic steps executed)
- Specifics code implementation (See method above)
- Input value (Measuring in term of input size)

Use [Random Access Machine](https://en.wikipedia.org/wiki/Random-access_machine) (RAM) as computation model to measure basic steps executed,

- Steps are executed sequentially
- Step is an operation that takes constant time (assignment, comparison, arithmetic operation, accessing object in memory)

Different input will result different output which affect measuring program complexity. The cases for measuring complexity are ([Best, worst and average case](https://en.wikipedia.org/wiki/Best,_worst_and_average_case)),

- $$\Omega$$ ([Omega](https://en.wikipedia.org/wiki/Big_Omega_function)), best case/lower bound, minimum running time over all possible input of a give size
- $$O$$ ([Big O](https://en.wikipedia.org/wiki/Big_O_notation)), worst case/upper bound, maximum running time over all possible input of given size
- Average/Expected case, average running time over all possible inputs of give size

The multiplicative constants are not relevant when comparing algorithms (`5n + 2` is basically as same as `n`), focus on the largest factor in the expression. Generally developers mostly concerned with the $$\Omega$$ (worst case) scenario.

[Asymptotic complexity](https://en.wikipedia.org/wiki/Asymptotic_computational_complexity),

- Describe running time in terms of number of basic steps
- If running time is sum of multiple terms, keep one with largest growth rate
- If remaining term is a product, drop any multiplicative constants

Complexity classes (Best to worst running time, examples can be found at [here](https://en.wikipedia.org/wiki/Big_O_notation#Orders_of_common_functions)),

- $$O(1)$$, constant
- $$O(log~n)$$, logarithmic (ex. Bisection search)
- $$O(n)$$, linear
- $$O(n~log~n)$$, log-linear (ex. [Merge sort](https://en.wikipedia.org/wiki/Merge_sort))
- $$O(n^c)$$, polynomial ($$c$$ is a constant)
- $$O(c^n)$$, exponential ($$c$$ is a constant being raised to a power based on size of input)

A logarithmic algorithm is often almost as good as a constant time algorithm, because the costs grow very slowly.

```
Constant > Log > Linear > Log-linear > Quadratic > Exponential
```

![](/images/big-o-complexity.png)
